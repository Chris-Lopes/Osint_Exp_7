# OSINT Pipeline

> Automated data collection pipeline for gathering Open Source Intelligence from multiple social media platforms.

## ğŸ“‹ Overview

This project demonstrates an automated OSINT (Open Source Intelligence) pipeline that collects, processes, and stores publicly available data from social media platforms including Reddit, GitHub, and Instagram.

## ğŸš€ Features

- **Multi-platform data collection** from Reddit and GitHub APIs
- **Language filtering** to collect English content only
- **Data normalization** across different platform formats
- **Error handling** with graceful degradation
- **Modular architecture** for easy platform integration

## ğŸ› ï¸ Technologies

- **Python 3.13.7**
- **PRAW** - Reddit API wrapper
- **PyGithub** - GitHub API integration
- **langdetect** - Language detection
- **Virtual environment** for dependency isolation

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone <repository-url>
cd osint_pipeline

# Create virtual environment
python -m venv myenv
source myenv/bin/activate  # On Windows: myenv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## âš™ï¸ Configuration

Create a `.env` file in the project root:

```env
# Reddit API Credentials
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_client_secret
REDDIT_USER_AGENT=your_user_agent

# GitHub API Token (optional, increases rate limits)
GITHUB_TOKEN=your_github_token
```

## ğŸ¯ Usage

Run the main pipeline:

```bash
python main.py
```

The pipeline will:
1. Collect data from Reddit (âœ…)
2. Collect data from GitHub (âœ…)
3. Attempt Instagram collection (âŒ fails due to API restrictions)
4. Filter for English content
5. Store normalized data

## ğŸ“‚ Project Structure

```
osint_pipeline/
â”œâ”€â”€ main.py              # Main execution script
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ cleaner.py       # Data filtering and cleaning
â”œâ”€â”€ config/              # Configuration files
â”œâ”€â”€ data/                # Collected data storage
â”œâ”€â”€ myenv/               # Virtual environment
â””â”€â”€ requirements.txt     # Python dependencies
```

## âš ï¸ Known Limitations

### Instagram Collection Failed
- **Instaloader library is no longer functional** due to Instagram's GraphQL API restrictions
- Instagram actively blocks automated scraping attempts
- Returns `403 Forbidden` and `401 Unauthorized` errors
- **Success rate: 0%**

### Recommendations:
- Use official Instagram API (requires app approval)
- Focus on Reddit and GitHub for reliable data collection
- Consider alternative platforms (Twitter, Telegram)

## ğŸ“Š Sample Results

**Successful Collections:**
- Reddit: 10 records from subreddits
- GitHub: 11 records from repositories
- **Total: 21 records collected**

**Failed Collections:**
- Instagram: 0 records (API deprecated)

## ğŸ”§ Troubleshooting

**Language Detection Errors:**
```python
langdetect.lang_detect_exception.LangDetectException: No features in text.
```
- Caused by empty text fields
- Handled gracefully with try-except blocks

**API Rate Limits:**
- Reddit: 60 requests/min (unauthenticated)
- GitHub: 60 requests/hour (unauthenticated), 5000/hour (authenticated)

## ğŸ“ Educational Purpose

This project was created for academic purposes to demonstrate:
- API integration and data collection
- ETL (Extract, Transform, Load) processes
- Error handling in data pipelines
- Ethical OSINT practices

## ğŸ“ License

This project is for educational purposes only. Always respect platform Terms of Service and privacy policies when collecting data.

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Open a pull request

## âš¡ Future Improvements

- [ ] Add PostgreSQL database integration
- [ ] Implement data visualization dashboard
- [ ] Add more platforms (Twitter, Telegram)
- [ ] Sentiment analysis on collected text
- [ ] Export to CSV/JSON formats
- [ ] Scheduled data collection

---

**Author**: Chris Lopes